{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2874558-010b-42e4-b756-c3e0ab1ed9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for PACT data processing in transfer to Box\n",
    "#import datatools\n",
    "\n",
    "import os\n",
    "# \"dev\" matches the name in ~/.aws/config\n",
    "os.environ['AWS_PROFILE'] = 'dev'\n",
    "os.environ['AWS_DEFAULT_PROFILE'] = 'dev'\n",
    "os.environ['HTTP_PROXY'] = 'http://proxy.sandia.gov:80'\n",
    "os.environ['HTTPS_PROXY'] = 'http://proxy.sandia.gov:80'\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/Users/jsstein/Documents/SNL_Root_CA.crt'\n",
    "os.environ['AWS_CA_BUNDLE'] = '/Users/jsstein/Documents/SNL_Root_CA.crt'\n",
    "from tqdm import tqdm\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('pvivdb-transfer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea905a-95ac-4506-a929-92ae347aacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in module setup file and generates dictionaries and variables\n",
    "def read_pact_modules():\n",
    "    import pandas as pd\n",
    "    modules = pd.read_csv('PACT_SNL_Outdoor_Modules_SETUP.csv')\n",
    "    active_modules = modules.loc[modules['Active']=='Y',:]\n",
    "    # Initialize dictionaries\n",
    "    allsamples = {}\n",
    "    allareas = {}\n",
    "    module_types = {}\n",
    "\n",
    "    # Loop through the DataFrame rows\n",
    "    for index, row in active_modules.iterrows():\n",
    "        pact_id = row['PACT_id']\n",
    "        psel_id = row['PSEL_id']\n",
    "        area = row['Area']\n",
    "        module_type = row['Type']\n",
    "        start_date = row['Start_date']\n",
    "        end_date = row['End_date']\n",
    "        notes = row['Notes']\n",
    "        \n",
    "        # Assign values to dictionaries\n",
    "        allsamples[pact_id] = psel_id\n",
    "        allareas[pact_id] = area\n",
    "        module_types[pact_id] = module_type\n",
    "\n",
    "    # Inverse the allsamples dictionary\n",
    "    pvid_to_pact = {v: k for k, v in allsamples.items()}\n",
    "    \n",
    "    # Generate the batches dictionary\n",
    "    # Initialize the batches dictionary\n",
    "    batches = {}\n",
    "\n",
    "    # Loop through the DataFrame rows\n",
    "    for index, row in active_modules.iterrows():\n",
    "        pact_id = row['PACT_id']\n",
    "        \n",
    "        # Check if the PACT_ID starts with 'P-' and extract the prefix\n",
    "        if pact_id.startswith('P-'):\n",
    "            prefix = pact_id[:6]  # Get the first 6 characters (e.g., 'P-0003')\n",
    "            \n",
    "            # Add the PACT_ID to the corresponding prefix list in the batches dictionary\n",
    "            if prefix not in batches:\n",
    "                batches[prefix] = []\n",
    "            batches[prefix].append(pact_id)\n",
    "    \n",
    "    # Initialize the current_modules dictionary\n",
    "    current_modules = {}\n",
    "    \n",
    "    # Loop through the active_modules DataFrame rows to populate current_modules\n",
    "    for index, row in active_modules.iterrows():\n",
    "        start_date = row['Start_date']\n",
    "        #end_date = row['End_date']\n",
    "        pact_id = row['PACT_id']\n",
    "        #area = row['Area']\n",
    "        #active = row['Active']\n",
    "        \n",
    "        current_modules[pact_id] = {'start': start_date, 'end': 'NAN'}\n",
    "    \n",
    "    # Initialize the batches dictionary\n",
    "    batches2 = {}\n",
    "    \n",
    "    # Loop through the active_modules DataFrame rows to populate batches\n",
    "    for index, row in active_modules.iterrows():\n",
    "        pact_id = row['PACT_id']\n",
    "        \n",
    "        # Extract the prefix (first 6 characters)\n",
    "        prefix = pact_id[:6]\n",
    "        \n",
    "        # Initialize the prefix in batches if not already present\n",
    "        if prefix not in batches2:\n",
    "            batches2[prefix] = {'modules': []}\n",
    "        \n",
    "        # Append the current PACT_ID to the corresponding prefix's modules list\n",
    "        batches2[prefix]['modules'].append(pact_id)\n",
    "    return active_modules, allsamples, allareas, module_types, batches, batches2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08621120-8034-4823-bc91-b0e3ff4104f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Module Metadata json files\n",
    "def make_module_metadata_json(path, batch, batches, allareas, module_types):\n",
    "    mod = []\n",
    "    for module in batches[batch]:\n",
    "        mod_dict = {'module_id': module,\n",
    "                    'module_area': allareas[module],\n",
    "                    'module_type': module_types[module],\n",
    "                    'days_indoors': [],\n",
    "                    'days_censored': []}\n",
    "        mod.append(mod_dict)\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(mod, outfile) \n",
    "    print(batch + ': Generated module-metadata.json')\n",
    "\n",
    "# module_id = 'P-0888-01'\n",
    "# path = '/Users/jsstein/bin/Box Sync copy/PACT - Data/'\n",
    "# metadata_path = path + module_id[:6] + '-XX/Outdoor_SNL/data/metadata/module-metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dfc18-4215-44e2-8446-29debd51b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate site-metadata.json\n",
    "def make_site_metadata_json(site_path, label, latitude, longitude, elevation, surface_tilt, surface_azimuth):\n",
    "    site_dict = {\"location\":{\"label\":label, \n",
    "                             \"latitude\":latitude, \n",
    "                             \"longitude\":longitude, \n",
    "                             \"elevation\":elevation,\n",
    "                             \"surface_tilt\":surface_tilt, \n",
    "                             \"surface_azimuth\":surface_azimuth}, \n",
    "                             \"snow_days\":[]}\n",
    "    with open(site_path, 'w') as outfile:\n",
    "        json.dump(site_dict, outfile) \n",
    "    print('Generated site-metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1fc58-dbfc-4ac5-8928-1086dfaa2301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to list modules in module_metadata.json\n",
    "\n",
    "def modules_from_metadata(path, batch):\n",
    "    import json\n",
    "    modules = []\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        for module_data in data:\n",
    "            modules.append(module_data['module_id'])\n",
    "    return modules\n",
    "\n",
    "#Testing the function\n",
    "# batch = 'P-0042'\n",
    "# path = '/Users/jsstein/bin/Box Sync/PACT - Data/'\n",
    "# module_list = modules_from_metadata(path, batch)\n",
    "# module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8074c09-a085-4cd2-b2cc-d26cb11a279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if module is in metadata\n",
    "def check_for_new_module(pact_id, path):\n",
    "    #metadata_path = path + pact_id[:6] + '-XX/Outdoor_SNL/data/metadata/module-metadata.json'\n",
    "    module_list = modules_from_metadata(path, pact_id[:6])\n",
    "    if pact_id in module_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Test function\n",
    "# path = '/Users/jsstein/bin/Box Sync/PACT - Data/'\n",
    "# pact_id = 'P-0042-065'\n",
    "# check_for_new_module(pact_id, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aabb19-e5c0-4c5c-ab47-f13f4cf16d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add a module to metadata.json\n",
    "# Assumes that pact_id is in the main module list with \n",
    "def add_module_to_metadata(path, pact_id):\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    if check_for_new_module(pact_id, path):\n",
    "        print(pact_id + ' is in module-metadata.json')\n",
    "    else:\n",
    "        pact_dict = {'module_id':pact_id, \n",
    "                     'module_area':allareas[pact_id], \n",
    "                     'module_type': module_types[pact_id],\n",
    "                     'days_indoors': [],\n",
    "                     'days_censored': []}\n",
    "        data.append(pact_dict)\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(data, outfile) \n",
    "        print(pact_id + ' added to module-metadata.json')\n",
    "    return data\n",
    "\n",
    "# path = '/Users/jsstein/bin/Box Sync/PACT - Data/'\n",
    "# batch = 'P-0138'\n",
    "# add_module_to_metadata(path, batch, 'P-0138-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e111940-b144-4dd8-af5c-5c178a1e8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def check_and_add_censor_condition(file_path, pact_id, start, end, comment):\n",
    "    # Load the existing JSON data from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Define the new censor condition\n",
    "    new_censor = {\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'comment': comment\n",
    "    }\n",
    "\n",
    "    # Check if the censor condition already exists\n",
    "    for row in data:\n",
    "        #print(row)\n",
    "        if row['module_id'] == pact_id:\n",
    "            # Check if the censor condition is already present\n",
    "            for censor in row['days_censored']:\n",
    "                if (censor['start'] == new_censor['start'] and\n",
    "                    censor['end'] == new_censor['end'] and\n",
    "                    censor['comment'] == new_censor['comment']):\n",
    "                    print(pact_id + \": Censored day condition already exists.\")\n",
    "                    return\n",
    "\n",
    "            # If not found, add the new censor condition\n",
    "            row['days_censored'].append(new_censor)\n",
    "            print(pact_id + \": Censored day condition added.\")\n",
    "\n",
    "            # Save the updated data back to the file\n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "            return\n",
    "\n",
    "    print('check_and_add_censor_condition: Module ID not found.')\n",
    "\n",
    "# Example usage\n",
    "#check_and_add_censor_condition('path_to_your_file.json', 'P-0042-01', \n",
    "#    '2024-10-01', '2024-10-02', 'new monitoring system outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd53f09-910a-414f-94bb-c71477c5233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_censored_days(censored_days_path):\n",
    "    with open(censored_days_path, 'r') as file:\n",
    "        censordata = pd.read_csv(file)\n",
    "    for index, row in censordata.iterrows():\n",
    "        pact_id = row['pact_id']\n",
    "        start_date = row['start']\n",
    "        end_date = row['end']\n",
    "        comment = row['comment']\n",
    "        \n",
    "        # Check the module-metadata.json to see if this censordata is included\n",
    "        if pact_id != 'site':\n",
    "            module_metadata_path = '/Users/jsstein/bin/Box Sync/PACT - Data/' + pact_id[:6] + '-XX/Outdoor_SNL/data/metadata/module-metadata.json'\n",
    "            check_and_add_censor_condition(module_metadata_path, pact_id, start_date, end_date, comment)\n",
    "        else:\n",
    "            # \"site\" in place of pact_id: Need to add to all modules active during period.\n",
    "            ###### NEED TO UPDATE THIS SECTION (JSS:11/2/25)\n",
    "            print('SITE Condition found')\n",
    "            for index, row in active_modules.iterrows():\n",
    "                pact_id = row['PACT_id']\n",
    "                module_metadata_path = '/Users/jsstein/bin/Box Sync/PACT - Data/' + pact_id[:6] + '-XX/Outdoor_SNL/data/metadata/module-metadata.json'\n",
    "                check_and_add_censor_condition(module_metadata_path, pact_id, start_date, end_date, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfba873e-ed29-42aa-8137-bd29052e0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_module(df, pact_id, start_date, end_date):\n",
    "       \n",
    "#     #Filter out modules other than pact_id\n",
    "#     dfmod = df.loc[df['ModuleID']==pact_id,:]\n",
    "#     print(pact_id + ': Latest TmStamp = ' + df.index[-1].strftime('%Y-%m-%d %H:%M') + ' Testpad = ' + str(df['TestPad'].iloc[-1]) )\n",
    "    \n",
    "#     if df['TestPad'].iloc[-1] == 4: \n",
    "#         # TestPad 4: East Pact Tracker (PACTTracker1)\n",
    "#         tablename = 'dbo.PACT_MET_PACTTracker1'\n",
    "#         sql = f\"SELECT TmStamp, E_Tracker1_Wm2_Avg, Trkr1Azimuth, Trkr1Altitude FROM {tablename} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#         df_tracker1 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#         df_tracker1 = df_tracker1.rename(columns={'E_Tracker1_Wm2_Avg': 'poa_global', 'Trkr1Azimuth': 'surface_azimuth', 'Trkr1Altitude':'surface_tilt'})\n",
    "#         df_tracker1 = df_tracker1.rename_axis('date_time')\n",
    "#         df_tracker1.index = df_tracker1.index.tz_localize('MST')\n",
    "        \n",
    "#         #Reading PACT_MET_30s\n",
    "#         tablename = 'dbo.PACT_MET_PACT_MET_30s'\n",
    "#         sql = f\"SELECT TmStamp,AmbientTemp_C_Avg  FROM {tablename} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#         df_met30s = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#         df_met30s = df_met30s.rename(columns={'AmbientTemp_C_Avg': 'temperature_air'})\n",
    "#         df_met30s = df_met30s.rename_axis('date_time')\n",
    "#         df_met30s.index = df_met30s.index.tz_localize('MST')\n",
    "\n",
    "#         #Concat all columns in correct order\n",
    "#         #print('TP=4:' + df_tracker1.columns + df_met30s.columns)\n",
    "#         if 'df_tracker1' in locals():\n",
    "#             df_tracker1 = df_tracker1[~df_tracker1.index.duplicated()]\n",
    "#         df_met30s = df_met30s[~df_met30s.index.duplicated()]\n",
    "#         df = df[~df.index.duplicated()]\n",
    "#         df_all = pd.concat([df_tracker1['poa_global'],df_met30s['temperature_air'],df['temperature_module'],\n",
    "#                     df['vmp'],df['imp'],df_tracker1['surface_tilt'],df_tracker1['surface_azimuth']], axis=1)\n",
    "    \n",
    "#     if df['TestPad'].iloc[-1] == 5: \n",
    "#         # TestPad 5: West Pact Tracker (PACTTracker2)\n",
    "#         tablename = 'dbo.PACT_MET_PACTTracker2'\n",
    "#         sql = f\"SELECT TmStamp, E_Tracker2_Wm2_Avg, Trkr2Azimuth, Trkr2Altitude FROM {tablename} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#         df_tracker2 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#         df_tracker2 = df_tracker2.rename(columns={'E_Tracker2_Wm2_Avg': 'poa_global', 'Trkr2Azimuth': 'surface_azimuth', 'Trkr2Altitude':'surface_tilt'})\n",
    "#         df_tracker2 = df_tracker2.rename_axis('date_time')\n",
    "#         df_tracker2.index = df_tracker2.index.tz_localize('MST')\n",
    "        \n",
    "#         #Reading PACT_MET_30s\n",
    "#         tablename = 'dbo.PACT_MET_PACT_MET_30s'\n",
    "#         sql = f\"SELECT TmStamp,AmbientTemp_C_Avg  FROM {tablename} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#         df_met30s = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#         df_met30s = df_met30s.rename(columns={'AmbientTemp_C_Avg': 'temperature_air'})\n",
    "#         df_met30s = df_met30s.rename_axis('date_time')\n",
    "#         df_met30s.index = df_met30s.index.tz_localize('MST')\n",
    "\n",
    "#         #Concat all columns in correct order\n",
    "#         #print('TP=5:' + df_tracker2.columns + df_met30s.columns)\n",
    "#         if 'df_tracker2' in locals():\n",
    "#             df_tracker2 = df_tracker2[~df_tracker2.index.duplicated()]\n",
    "#         df_met30s = df_met30s[~df_met30s.index.duplicated()]\n",
    "#         df = df[~df.index.duplicated()]\n",
    "#         df_all = pd.concat([df_tracker2['poa_global'],df_met30s['temperature_air'],df['temperature_module'],\n",
    "#                     df['vmp'],df['imp'],df_tracker2['surface_tilt'],df_tracker2['surface_azimuth']], axis=1)\n",
    "        \n",
    "#         #####Adjust for bias caused by load switching#######\n",
    "#         if pact_id == 'P-0042-03':\n",
    "#             df_all['vmp'] = df_all['vmp']*1.14\n",
    "#         if pact_id == 'P-0042-04':\n",
    "#             df_all['vmp'] = df_all['vmp']*1.14\n",
    "\n",
    "#     elif df['TestPad'].iloc[-1] == 6:\n",
    "#         # TestPad 6: East Fixed Tilt\n",
    "\n",
    "#         if int(start_date[0:4] + start_date[5:7])== 202512:\n",
    "#             # Added logic to deal with moving POA out of PACT_MET table in December 2025.\n",
    "#             tablename1 = 'dbo.PACT_MET_PACT_MET_30s'\n",
    "#             tablename2 = 'dbo.PACT_MET_PACTWestTilt_30s'\n",
    "#             sql = f\"SELECT TmStamp,E_TiltPOA_Wm2_Avg,AmbientTemp_C_Avg  FROM {tablename1} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#             df_met30s_1 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#             df_met30s_1.rename(columns={'E_TiltPOA_Wm2_Avg': 'E_WestTiltPOA_Wm2_Avg'}, inplace=True)\n",
    "#             sql = f\"SELECT TmStamp,E_WestTiltPOA_Wm2_Avg  FROM {tablename2} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#             df_met30s_2 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#             #Merge tables\n",
    "#             df_met30s = dfmet30s_1['E_WestTiltPOA_Wm2_Avg'].combine_first(df_met30s_2['E_WestTiltPOA_Wm2_Avg'])\n",
    "#             df_met30s = df_met30s.rename(columns={'E_WestTiltPOA_Wm2_Avg': 'poa_global','AmbientTemp_C_Avg': 'temperature_air'})\n",
    "#             df_met30s = df_met30s.rename_axis('date_time')\n",
    "#             df_met30s.index = df_met30s.index.tz_localize('MST')\n",
    "        \n",
    "#         elif int(start_date[0:4] + start_date[5:7])< 202512:\n",
    "#             # For all months BEFORE Dec 2025\n",
    "#             tablename = 'dbo.PACT_MET_PACT_MET_30s'\n",
    "#             sql = f\"SELECT TmStamp,E_TiltPOA_Wm2_Avg,AmbientTemp_C_Avg  FROM {tablename} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#             df_met30s = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#             df_met30s = df_met30s.rename(columns={'E_TiltPOA_Wm2_Avg': 'poa_global','AmbientTemp_C_Avg': 'temperature_air'})\n",
    "#             df_met30s = df_met30s.rename_axis('date_time')\n",
    "#             df_met30s.index = df_met30s.index.tz_localize('MST')\n",
    "\n",
    "#         elif int(start_date[0:4] + start_date[5:7])> 202512:\n",
    "#             # For all months AFTER Dec 2025\n",
    "#             #tablename1 = 'dbo.PACT_MET_PACT_MET_30s'\n",
    "#             tablename2 = 'dbo.PACT_MET_PACTWestTilt_30s'\n",
    "#             #sql = f\"SELECT TmStamp,AmbientTemp_C_Avg  FROM {tablename1} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#             #df_met30s_1 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#             sql = f\"SELECT TmStamp,E_WestTiltPOA_Wm2_Avg  FROM {tablename2} WHERE TmStamp BETWEEN '{start_date}' AND '{end_date}' ORDER BY TmStamp ASC\"\n",
    "#             df_met30s_2 = pd.read_sql(sql, engine, index_col=\"TmStamp\")\n",
    "#             #Merge tables\n",
    "#             #df_met30s = dfmet30s_1['E_EastTiltPOA_Wm2_Avg'].combine_first(df_met30s_2['E_EastTiltPOA_Wm2_Avg'])\n",
    "#             df_met30s = df_met30s.rename(columns={'E_WestTiltPOA_Wm2_Avg': 'poa_global','AmbientTemp_C_Avg': 'temperature_air'})\n",
    "#             df_met30s = df_met30s.rename_axis('date_time')\n",
    "#             df_met30s.index = df_met30s.index.tz_localize('MST')\n",
    "    \n",
    "#         df_met30s['surface_tilt'] = np.zeros(len(df_met30s))+35\n",
    "#         df_met30s['surface_azimuth'] = np.zeros(len(df_met30s))+180\n",
    "    \n",
    "#         #Concat all columns in correct order\n",
    "#         #print('TP=6:' + df_met30s.columns)\n",
    "#         if 'df_tracker2' in locals():\n",
    "#             df_tracker2 = df_tracker2[~df_tracker2.index.duplicated()]\n",
    "#         df_met30s = df_met30s[~df_met30s.index.duplicated()]\n",
    "#         dfmod = dfmod[~dfmod.index.duplicated()]\n",
    "#         df_all = pd.concat([df_met30s['poa_global'],df_met30s['temperature_air'],dfmod['temperature_module'],\n",
    "#                     dfmod['vmp'],dfmod['imp'],df_met30s['surface_tilt'],df_met30s['surface_azimuth']], axis=1)\n",
    "#     else:\n",
    "#         print(pact_id + ': TestPad =' + str(dfmod['TestPad'][-1]) + ' not recognized')\n",
    "    \n",
    "        \n",
    "#     path = '/Users/jsstein/bin/Box Sync/PACT - Data/'+ pact_id[:6] + '-XX/Outdoor_SNL/data/point-data/'\n",
    "#     df_all.to_csv(path +'point-data_' + pact_id + '_' + yearmonth + '.csv', index=True)\n",
    "    \n",
    "#     # # Write file to BOX\n",
    "#     # client = make_client()\n",
    "#     # folders = get_key_folders(pact_id[0:6], client)\n",
    "#     # datafile = path + 'point-data_' + pact_id + '_' + str(start_date)[0:7] + '.csv'\n",
    "#     # upload_file(datafile, folders['point-data'], client)\n",
    "#     # print(datafile + ' copied to Box')\n",
    "\n",
    "#     #Write file to AWS S3\n",
    "#     s3_key = pact_id[:6] + '-XX/Outdoor_SNL/data/point-data/' + 'point-data_' + pact_id + '_' + yearmonth + '.csv'\n",
    "#     bucket.upload_file(path + 'point-data_' + pact_id + '_' + str(start_date)[0:7] + '.csv' ,s3_key)\n",
    "#     print(s3_key + ' copied to AWS S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a5b9cb-3111-47c6-b13b-13c37bf77e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = '2025-12-01'\n",
    "int(start_date[0:4] + start_date[5:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce8d23-8d8e-489d-bcbe-596fef4d9782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
